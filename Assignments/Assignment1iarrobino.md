What responsibilities do software engineers have in mitigating the environmental and social impacts of large language models (LLMs)? Refer to: “On the Dangers of Stochastic Parrots” and “Challenging the Myths of Generative AI”


Large language models (LLMs) can have many negative impacts, including but not limited to generating false information, perpetuating biases, and contributing to carbon emissions and climage change. As engineers, we have the responsibility of mitigating these impacts as much as we can. The easiest way to mitigate these impacts is to not abuse them, it's easy to be lazy and have AI generate anything you need, but it's also wasteful. It's important to not become complicit in the laziness and actually do the work you are supposed to do. On top of that we should also be vetting information, do not immedietly trust an answer AI generates for you, we should checking sources and even trying a good old fashioned google search before asking AI. Lastly on the topic of bias, AI is influencing our thoughts and thought processes, it's imporatant to understand this becuase there are biases baked into these models that will influence our own biases if we rely on them too heavily. This will have incredibly negative social impacts if we allow AI to do the thinking for us. Basically - don't be lazy and continue to use your own brain to think.