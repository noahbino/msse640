Discuss: Easy problems that are hard for LLMs.

One way to get a better understanding of what LLMs can and cannot do is to look at some of the seemingly simple questions that trip them up.
Find some examples by experimenting or searching online.
Discuss why these problems are hard for LLMs even after several years of intensive effort to improve their performance â€” and what that tells us about their capacities now and in the future.

# Response 

There are many simple problems that are hard for LLMS, and a lot of the time it depends on the context of the query. For example, a popular basic math question like 'If you have 5 apples and you give 2 away, how many do you have left?' is a question that can trip up an LLM due to the ownership and verb tense of the question. 

# ChatGPT answer:
Great one ğŸ˜„ Hereâ€™s the twist:
	â€¢	If youâ€™re asking how many apples you still possess after giving away 2, the answer is:
ğŸ‘‰ 3 apples

BUTâ€¦
	â€¢	If youâ€™re asking literally how many apples you have ever had (regardless of giving them away), itâ€™s:
ğŸ‘‰ 5 apples

Although it does include the correct answer, it provides way too much extra detail when the simplest and most correct answer is just "3".

Another type of question that LLMs can struggle with are common sense exceptions, things that humans can easily understand and identify can be difficult for an LLM that simply doesn't have human reasoning for common sense. I asked ChatGPT if alligators can play the piano

# ChatGPT answer:

ğŸ˜„ Fun one â€” hereâ€™s the breakdown:

â¸»

ğŸ§  Literal Answer:

No, a crocodile cannot play the piano.

Why? Because:
	â€¢	ğŸŠ Crocodiles donâ€™t have the dexterity or anatomical structure to press keys in a controlled way
	â€¢	They lack training capacity, motivation, and, wellâ€¦ fingers

â¸»

ğŸ¤– Why LLMs Might Say â€œYesâ€

Some LLMs might respond:

â€œYes, if itâ€™s trainedâ€
or
â€œTechnically, it could hit keys randomlyâ€

This happens because LLMs donâ€™t understand physical constraints â€” they simulate plausibility based on language patterns, not grounded knowledge of crocodile biology ğŸ§¬ğŸ¹.

Again, since LLMs don't have a sense of reality they can try to find technically correct answers without regard to common sense. 


I believe this will be the next big step with artifical intelligence, basic human common sense. As simple as common sense may seem, it's certaintly very difficult to code into a program becuase sometimes it requires the system to abandon the answering the question in a deeply analytical way, which is what LLMs are currently trained to do. Finding that balance between the 2 will not be an easy task. 